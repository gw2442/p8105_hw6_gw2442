---
title: "p8106_hw6_gw2442"
output: github_document
date: "2022-11-28"
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(broom)
library(patchwork)
library(dplyr)
library(modelr)
library(mgcv)
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

## Problem 2

#### Importing and describing raw data: 

```{r}
homicide_data = read_csv(file = "./data/homicide-data.csv") %>%
  janitor::clean_names()
skimr::skim(homicide_data)
```

The CSV file was downloaded from the GitHub repository, imported, and cleaned. The dataset `homicide_data` has 52,179 rows and 12 columns. The dataset contains information on the homicide ID number, reported date, the victim's first and last name, race, age, sex, location of homicide (including city, state, longtidue, and latitude), and disposition of the case. While there are 0 missing variables for variables `uid`, `victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `disposition`, and `reported date`, there are 60 missing variables for `lat` and `lon`. 


#### Creating variable `city_state`:

```{r}
homicide_data =
  homicide_data %>%
  mutate(
    city_state = as.character(paste(city, state, sep = "_")))
```


#### Creating variable `unsolved_homicide` and tidying data: 

The variable `solved_homicide` was created. Those with the disposition "Closed without arrest" or "Open/No arrest" were categorised as an unsolved homicide and were therefore given a value of 0 in the variable `solved_homicide`. Those with the disposition "Closed by arrest" were categorised as a solved homicide and were therefore given a value of 1 in the variable `solved_homicide`. 

The cities Dallas, Phoenix, Kansas City, and Tulsa were omitted from the dataset, as well as any race entries other than White or Black and any victim age's that were non-numeric. 

```{r}
homicide_data = 
  homicide_data %>%
  mutate(solved_homicide = ifelse(disposition == "Closed by arrest", 1, 0)) %>%
  filter(city_state != "Dallas_TX", 
         city_state != "Phoenix_AZ", 
         city_state != "Kansas City_MO", 
         city_state != "Tulsa_AL") %>%
  filter(victim_race == "White" | victim_race == "Black") %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  drop_na(victim_age)
```


#### Logistic regression on Baltimore, MD

A logistic regression was run for the city of Baltimore with resolved vs unresolved as the outcome and victim age, sex, and race as predictors using the `glm` function. The output was tidied using `broom::tidy`. That output was used to determine the adjusted odds ratio and corresponding 95% confidence interval (test statistics of 1.96) for the city of Baltimore. 

```{r}
fit_logistic_baltimore = 
  homicide_data %>%
  filter(city == "Baltimore") %>%
  glm(solved_homicide ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
  broom::tidy()

fit_logistic_baltimore %>%
  mutate(OR = exp(estimate),
         CI_lower = exp(estimate - 1.96*std.error),
         CI_upper = exp(estimate + 1.96*std.error)) %>%
  select(term, OR, CI_lower, CI_upper) %>%
  filter(term == "victim_sexMale")
```

The adjusted odds ratio for solving homicides comparing male victims to female victims is 0.426 with a 95% confidence interval of (0.325, 0.558), keeping all other variables fixed. 

In the city of Baltimore, the odds of solving a homicide with a male victim is 0.426 times the odds of solving a homicide with a female victim, keeping all other variables fixed. 


#### Mapping logistic regression for all cities

The function `fit_logsitic_homicide` was created. It produces the adjusted odds ratio for sovling homicides comparing male victims to female victims and its corresponding 95% confidence interval. The function was then mapped onto the nested data frame `homicide_data_analysis`. The data frame was then unnested to obtain a final data frame demonstrating each `city_state` and its corresponding OR and CI. 

```{r}
fit_logistic_homicide = function(homicide_data) {
  glm(solved_homicide ~ victim_age + victim_race + victim_sex, data = homicide_data, family = binomial()) %>%
    broom::tidy() %>%
    mutate(OR = exp(estimate),
           CI_lower = exp(estimate - 1.96*std.error),
           CI_upper = exp(estimate + 1.96*std.error)) %>%
    select(term, OR, CI_lower, CI_upper) %>%
    filter(term == "victim_sexMale")
}

homicide_data_analysis = 
  homicide_data %>%
  select(city_state, everything()) %>%
  nest(data = uid:solved_homicide) %>%
  mutate(regression_result = purrr::map(.x = data, ~fit_logistic_homicide(.x))) %>%
  unnest(cols = regression_result)
```

#### Plotting OR and CI's

A plot demonstrating the estimated ORs and CIs for each city was created from the unnested data frame `homicide_data_analysis`. It is organised according to estimated OR.

```{r}
homicide_data_analysis %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) + 
  geom_errorbar(aes(ymin= CI_lower, ymax= CI_upper)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  labs(
        x = "Cities",
        y = "Odds Ratio",
        title = "Odds Ratio for Solving Homicides Comparing Male Victims to Female Victims")
```

The plot demonstrates that Albuquerque, NM has the highest odds for solving homicides with male victims compared to female victims. However, it's 95% CI is extremely wide, especially compared to the other cities in the dataset. New York, NY has the lowest odds for solving homicides with male victims compared to female victims. 


## Problem 3

#### Loading and cleaning data for regression analysis

```{r}
bw_data = read_csv(file = "./data/birthweight.csv") %>%
  janitor::clean_names()
skimr::skim(bw_data)

bw_data %>%
  distinct(babysex) 

bw_data %>%
  distinct(frace)

bw_data %>%
  distinct(malform)

bw_data %>%
  distinct(mrace)

bw_data =
  bw_data %>%
  mutate(
    babysex = factor(babysex, labels = c("male", "female")),
    frace = factor(frace, labels = c("white", "black", "asian", "puerto rican", "other")),
    malform = factor(malform, labels = c("absent", "present")),
    mrace = factor(mrace, labels = c("white", "black", "asian", "puerto rican")), 
  )

skimr::skim(bw_data)
```

Variables `babysex`, `frace`, `malform`, and `mrace` were converted to factor variables, as they were all categorical variables. Variable `frace` had no "Unknown" (9) inputs. Variable `mrace` had no "Other" (8) inputs. Therefore, they were omitted when labeling the factor variables. The function `skimr::skim` demonstrates that there are no missing data for any of the variables. 


#### Proposed regression model for birthweight

We hypothesise that a baby's birth weight is dependent on the baby's length at birth (cm). 

```{r}
bw_data %>%
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = 0.5)
```

Based on the plot above, baby's length at birth seems to be positively correlated with the baby's birth weight. 

To test this hypothesis, we run a regression model with birth weight as the outcome and baby's length at birth as a predictor. This model is illustrated in the plot below, with predictions on the x axis and residuals on the y axis. 

```{r}
fit = lm(bwt ~ blength, data = bw_data) 

bw_data = 
  bw_data %>%
  mutate(
    modelr::add_residuals(bw_data, fit),
    modelr::add_predictions(bw_data, fit))

bw_data %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point()

fit %>%
  broom::tidy() %>%
  knitr::kable()
```


#### Comparing models

The model above was compared with two models: 

* One using length at birth and gestational age as predictors (fit_2)

* One using head circumference, length, sex, and all interactions as predictors (fit_3)

Comparison Models:

```{r}
fit_2 = lm (bwt ~ blength + gaweeks, data = bw_data)

fit_3 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = bw_data)
```

These three models (`fit`, `fit_2`, and `fit_3`) were compared in terms of the cross-validated prediction error. The `crossv_mc` function was on the `bw_data` dataset to producing training and testing data sets. The candidate models were then fit and its corresponding RMSEs were obtained using `mutate` + `map` & `map2`.

```{r}
cv_df = 
  crossv_mc(bw_data, 100)

cv_df = 
  cv_df %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>%
  mutate(
    fit_mod   = map(train, ~lm(bwt ~ blength, data = .x)),
    fit_2_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit_3_mod = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>%
  mutate(
    rmse_fit   = map2_dbl(fit_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit_2 = map2_dbl(fit_2_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit_3 = map2_dbl(fit_3_mod, test, ~rmse(model = .x, data = .y))
  )
```

The plot below shows the distribution of RMSE values for each candidate model. 

```{r}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    x = "model",
    y = "rmse",
    title = "Distribution of RMSE values for each model"
    )
  
```

RMSE is a metric that ranges from 0 to infinity. Models with a lower value are better able to "fit" the data set. Based on the plot above, the fit_3 model (using head circumference, length, sex, and all interactions) would be most appropriate, as it demonstrates the lowest RMSE value. However, all models demonstrate an extremely high RMSE value, as RMSE values between 0.2 and 0.5 indicate a relatively accurate prediction model. The RMSE in all three of the models above greately exceed these numbers. 


